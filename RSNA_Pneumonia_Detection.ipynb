{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nles-yUX08Z"
      },
      "source": [
        "#0. Setting up the Environment\n",
        "Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foc3RGMqY0Xe",
        "outputId": "13aa65ff-e074-4f59-d58e-3f5a4fa74ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom\n",
            "  Using cached pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "Collecting torchmetrics\n",
            "  Using cached torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "Collecting pytorch_lightning\n",
            "  Using cached pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
            "  Using cached lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: pydicom, multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pydicom-2.3.1 pytorch_lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install pydicom torchmetrics pytorch_lightning\n",
        "! pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YevZJI0KVuZm"
      },
      "outputs": [],
      "source": [
        "# Imports for this project\n",
        "import json \n",
        "import os\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import pydicom\n",
        "from pydicom import dcmread\n",
        "\n",
        "import cv2\n",
        "import torchvision\n",
        "import torchmetrics\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6CZbd3svNE1"
      },
      "source": [
        "#1. Downloading Data\n",
        "\n",
        "\n",
        "In this section we download the data needed for our poject from Kaggle. The dataset can be found [here](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge).\n",
        "\n",
        "Work done by: Aidan Daly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43kHJSH4tp0O"
      },
      "outputs": [],
      "source": [
        "# API Username and Key\n",
        "api_key = {\n",
        "'username':\"aidandaly\" ,\n",
        "'key':\"9f430a42e2d60b123bead1ea61ad5617\"}\n",
        "\n",
        "#Sets up data download\n",
        "kaggle_path = Path('/root/.kaggle')\n",
        "os.makedirs(kaggle_path, exist_ok=True)\n",
        "with open (kaggle_path/'kaggle.json', 'w') as handl:\n",
        "  json.dump(api_key,handl)\n",
        "\n",
        "os.chmod(kaggle_path/'kaggle.json', 600) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aGR_9xRtM_v"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c rsna-pneumonia-detection-challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEoVje77uZif"
      },
      "outputs": [],
      "source": [
        "# Unzip downloaded file from kaggle into a new folder \"project\"\n",
        "with zipfile.ZipFile(\"rsna-pneumonia-detection-challenge.zip\", 'r') as zip_ref:\n",
        "  zip_ref.extractall(\"./project\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HajlWOVwGK4"
      },
      "outputs": [],
      "source": [
        "# Check to make sure everything was downloaded correctly\n",
        "print(os.listdir(\"./project\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4mpDJ5wW3QK"
      },
      "source": [
        "#2a. Visualizing Data\n",
        "\n",
        "In this section we will attempt to look through the data and gain a better understanding of it through some visualizations.\n",
        "\n",
        "Work done by: Aidan Daly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkXQMRwNrNCY"
      },
      "outputs": [],
      "source": [
        "label_data = pd.read_csv(\"./project/stage_2_train_labels.csv\")\n",
        "label_data.drop_duplicates()\n",
        "columns = ['patientId', 'Target']\n",
        "\n",
        "label_data = label_data.filter(columns)\n",
        "label_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHJj5WerCG1J"
      },
      "source": [
        "We can see the data is unbalanced, but we'll deal with that later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8zRqEeBx3IB"
      },
      "outputs": [],
      "source": [
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Here I change the label 0 to \"Pneumonia Free Tissue\" and 1 to \"Pneumonia Tissue\"\n",
        "pie_chart=pd.DataFrame(label_data['Target'].replace(0,'Pneumonia Free Tissue').replace(1,'Pneumonia Tissue').value_counts())\n",
        "pie_chart.reset_index(inplace=True)\n",
        "\n",
        "# Print out the chart\n",
        "pie_chart.plot(kind='pie', title='Image Labels',y = 'Target', \n",
        "             autopct='%1.1f%%', shadow=False, labels=pie_chart['index'], legend = True, fontsize=15, figsize=(18,8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HauPsq-14jL6"
      },
      "source": [
        "We can now visualize some of the data here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AfO7HSE2aWl"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 3, figsize=(10, 10))\n",
        "for i, axis in enumerate(ax.flat):\n",
        "  im_file = str(\"./project/stage_2_train_images/\" + label_data.patientId[i] + '.dcm')\n",
        "  im_dcm = dcmread(im_file)\n",
        "  axis.imshow(im_dcm.pixel_array, cmap=\"bone\")\n",
        "  axis.set(xticks=[], yticks=[], xlabel = label_data.Target[i]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDxi_v8YDbN8"
      },
      "source": [
        "#2b. Processing Data\n",
        "Below we begin to process our data. We split our training data into training and validation.\n",
        "\n",
        "Directly below you can see a class called `Dataset` this is something we implemented from a similar Kaggle project.\n",
        "\n",
        "Work Done By: Aidan Daly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx0PPc0jzNvQ"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, paths, labels=None, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = dcmread(f'{self.paths[index]}.dcm')\n",
        "        image = image.pixel_array\n",
        "        image = image / 255.0\n",
        "\n",
        "        image = (255*image).clip(0, 255).astype(np.uint8)\n",
        "        image = Image.fromarray(image).convert('RGB')\n",
        "\n",
        "        label = self.labels[index][1]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApAG_Sy8zxDM"
      },
      "outputs": [],
      "source": [
        "def balance(imgs):\n",
        "  remove_dups(imgs)\n",
        "  new_imgs = []\n",
        "  zero_count = 0\n",
        "  one_count = 0\n",
        "  for i in range(len(imgs)):\n",
        "    if imgs[i][1] == 0:\n",
        "      zero_count += 1\n",
        "    else:\n",
        "      one_count += 1\n",
        "  balancee = 0\n",
        "  if one_count > zero_count:\n",
        "    balancee = 1\n",
        "\n",
        "  diff = abs(zero_count - one_count)\n",
        "  removed = 0\n",
        "  for i in range(len(imgs)):\n",
        "    if imgs[i][1] != balancee or removed >= diff:\n",
        "      new_imgs.append(imgs[i])\n",
        "    else:\n",
        "      removed += 1\n",
        "\n",
        "  return np.asarray(new_imgs)\n",
        "\n",
        "\n",
        "def remove_dups(imgs):\n",
        "  new_imgs = []\n",
        "  seen = set()\n",
        "  for i in range(len(imgs)):\n",
        "    if imgs[i][0] not in seen:\n",
        "      seen.add(imgs[i][0])\n",
        "      new_imgs.append(imgs[i])\n",
        "  return np.asarray(new_imgs)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kiiE3cnoiiB"
      },
      "outputs": [],
      "source": [
        "train_f = './project/stage_2_train_images'\n",
        "test_f = './project/stage_2_test_images'\n",
        "\n",
        "#Split Data\n",
        "train_labels, val_labels = train_test_split(label_data.values, test_size=0.1)\n",
        "train_labels, test_labels = train_test_split(train_labels, test_size=0.01)\n",
        "print(f\"Trining Data Shape: {train_labels.shape}\")\n",
        "print(f\"Validation Data Shape: {val_labels.shape}\")\n",
        "\n",
        "train_labels = balance(train_labels)\n",
        "val_labels = balance(val_labels)\n",
        "test_labels= balance(test_labels)\n",
        "print(f\"Balanced Trining Data Shape: {train_labels.shape}\")\n",
        "print(f\"Balanced Validation Data Shape: {val_labels.shape}\")\n",
        "print(f\"Balanced Test Data Shape: {test_labels.shape}\")\n",
        "\n",
        "train_paths = [os.path.join(train_f, image[0]) for image in train_labels]\n",
        "val_paths = [os.path.join(train_f, image[0]) for image in val_labels]\n",
        "test_paths = [os.path.join(train_f, image[0]) for image in test_labels]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "\n",
        "#Check Dataset\n",
        "train_dataset = Dataset(train_paths, train_labels, transform=transform)\n",
        "image = iter(train_dataset)\n",
        "img, label = next(image)\n",
        "img = np.transpose(img, (1, 2, 0))\n",
        "plt.imshow(img)\n",
        "\n",
        "#Create Datasets and DataLoaders\n",
        "train_dataset = Dataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = Dataset(val_paths, val_labels, transform=transform)\n",
        "test_dataset = Dataset(test_paths, test_labels, transform = transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "grad_cam_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Kaggle Test Setup\n",
        "kaggle_labels = os.listdir('./project/stage_2_test_images')\n",
        "for i in range(len(kaggle_labels)):\n",
        "  kaggle_labels[i] = [kaggle_labels[i][:-4], 0]\n",
        "kaggle_paths = [os.path.join(test_f,image[0]) for image in kaggle_labels]\n",
        "\n",
        "kaggle_test_dataset = Dataset(kaggle_paths, kaggle_labels, transform = transform)\n",
        "test_dataset_loader = DataLoader(dataset = kaggle_test_dataset,  batch_size=32)\n"
      ],
      "metadata": {
        "id": "nt5oXYcb8WgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJtfLlLUCLG"
      },
      "source": [
        "#3. Model\n",
        "For this model we decided to use a commonly used Convolutional Neural Network (CNN) called ResNet18. We decided to essentially use the base model as it is sufficient enough for what we are trying to do.\n",
        "\n",
        "Our model is also inspired by a few different projects we found online, which we combined together with our own knowledge as well as previous works in Psets 3 and 4.\n",
        "\n",
        "Work done by: Aidan Daly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRZnz3OuUVzn"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "\n",
        "class GradCamModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gradients = None\n",
        "        self.tensorhook = []\n",
        "        self.layerhook = []\n",
        "        self.selected_out = None\n",
        "        \n",
        "        # MODEL\n",
        "        self.model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
        "        self.layerhook.append(self.model.layer4.register_forward_hook(self.forward_hook()))\n",
        "\n",
        "        \n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        \n",
        "    \n",
        "    def activations_hook(self,grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def get_act_grads(self):\n",
        "        return self.gradients\n",
        "\n",
        "    def forward_hook(self):\n",
        "        def hook(module, inp, out):\n",
        "            self.selected_out = out\n",
        "            self.tensorhook.append(out.register_hook(self.activations_hook))\n",
        "        return hook\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.model(x)\n",
        "        return out, self.selected_out\n",
        "\n",
        "\n",
        "#Model\n",
        "model = GradCamModel()\n",
        "model.to(device)\n",
        "\n",
        "#Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "#Optimization\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4TKkDIyZVSf"
      },
      "source": [
        "#4a. Training\n",
        "You must run the `check_accuracy()` function, but we do not recommend running training as it may use up all cuda memory which will be needed as we continue.\n",
        "\n",
        "Work done by: Luke Pisani and Aidan Daly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(model, loader):\n",
        "  print('Checking accuracy on current set')   \n",
        "  # Validation step\n",
        "  correct = 0\n",
        "  total = 0  \n",
        "  for images, labels in tqdm(loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      predictions, x = model(images)\n",
        "      _, predicted = torch.max(predictions, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (labels == predicted).sum()\n",
        "  val_acc= 100*correct/total\n",
        "  return val_acc"
      ],
      "metadata": {
        "id": "RRf0KLNY7vJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VygQaHhjQQWu"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "\n",
        "num_epochs = 20\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training step\n",
        "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs, x = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 2000 == 0:\n",
        "            \n",
        "            print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "    # Validation step\n",
        "    val_acc=check_accuracy(model, val_loader)\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}, Val_Acc: {val_acc}')\n",
        "    if epoch %5 == 0:\n",
        "      model_save_name = 'classifier.pth'\n",
        "      path = f\"drive/MyDrive/{model_save_name}\" \n",
        "      torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4b. Grad Cam Visualization\n",
        "We will now inspect what the grad cam heatmap looks like for specific images. It's clear to see that the heat map is looking in the area of the lungs."
      ],
      "metadata": {
        "id": "OBiFbdPg87Yl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkZYXI8ekuHf"
      },
      "outputs": [],
      "source": [
        "! gdown 1ctysbvJRG86cbDkha70ldUj6zCWS3UwL\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "from skimage.transform import resize\n",
        "\n",
        "# set the evaluation mode\n",
        "path = \"/content/classifier.pth\"\n",
        "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
        "model.eval()\n",
        "i = 0\n",
        "# get the image from the dataloader\n",
        "for images, labels in tqdm(grad_cam_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    img = images.cpu().detach().numpy()\n",
        "    img /= 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape((1,3,1,1))\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape((1,3,1,1))\n",
        "\n",
        "    out, acts = model(images)\n",
        "    i += 1\n",
        "    if i == 1:\n",
        "      break\n",
        "acts = acts.detach().cpu()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()(out,torch.from_numpy(np.array([600])).to(device))\n",
        "loss.backward()\n",
        "\n",
        "grads = model.get_act_grads().detach().cpu()\n",
        "\n",
        "pooled_grads = torch.mean(grads, dim=[0,2,3]).detach().cpu()\n",
        "\n",
        "for i in range(acts.shape[1]):\n",
        "    acts[:,i,:,:] *= pooled_grads[i]\n",
        "\n",
        "heatmap_j = torch.mean(acts, dim = 1).squeeze()\n",
        "heatmap_j_max = heatmap_j.max(axis = 0)[0]\n",
        "heatmap_j /= heatmap_j_max\n",
        "\n",
        "cmap = mpl.colormaps.get_cmap('plasma')\n",
        "heatmap_j2 = cmap(heatmap_j,alpha = 0.2)\n",
        "\n",
        "fig, axs = plt.subplots(1,1,figsize = (5,5))\n",
        "axs.imshow(((img*std+mean)[0].transpose(1,2,0)))\n",
        "axs.imshow(heatmap_j2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kbUFIdJGIOC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGdr_GGXfJB"
      },
      "source": [
        "#5. Testing\n",
        "Checking our acccuracy on our Test Dataset\n",
        "\n",
        "Author: Luke Pisani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkko5LcrZEk4"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = \"/content/classifier.pth\"\n",
        "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
        "acc =check_accuracy(model, test_loader)\n",
        "print(f\"\\nAccuracy on Test: {acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkN0shBtVJd"
      },
      "source": [
        "Here we create our sample submission for Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QITIaExCFiLb"
      },
      "outputs": [],
      "source": [
        "unlabeled = pd.read_csv('./project/stage_2_sample_submission.csv')\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i, (images, labels) in enumerate(tqdm(test_dataset_loader, total=int(len(test_dataset_loader)))):\n",
        "  images = images.to(device)\n",
        "  labels = labels.to(device)\n",
        "  \n",
        "  pred, outputs = model(images)\n",
        "  _, predicted = torch.max(pred, 1)\n",
        "  \n",
        "  for j in predicted:\n",
        "      predictions.append(j.item())\n",
        "\n",
        "\n",
        "unlabeled['PredictionString'] = predictions\n",
        "\n",
        "\n",
        "test_images = np.random.choice(unlabeled.patientId, size=50, replace=False)     \n",
        "\n",
        "fig, ax = plt.subplots(5, 10, figsize=(20,10))\n",
        "\n",
        "for n in range(5):\n",
        "  for m in range(10):\n",
        "    img_id = test_images[m + n*10]\n",
        "    image = dcmread(test_f + '/' + img_id + \".dcm\").pixel_array\n",
        "    pred = unlabeled.loc[unlabeled['patientId'] == img_id, 'PredictionString'].values[0]\n",
        "    label = \"Pneumonia\" if(pred >= 0.5) else \"Healthy\"  \n",
        "    ax[n,m].imshow(image, cmap=\"bone\")\n",
        "    ax[n,m].grid(False)\n",
        "    ax[n,m].tick_params(labelbottom=False, labelleft=False)\n",
        "    ax[n,m].set_title(\"Label: \" + label)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXKYUl0K33l7"
      },
      "source": [
        "#6. Filter Visualization\n",
        "Below we get a random image from the dataset and push it through the model, saving the filter map after each layer of convolution. This helps to get a sense of how each layer is modifying an input image.\n",
        "\n",
        "Code adapted from https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch-12a48cd1e573\n",
        "\n",
        "Work done by: Aidan Daly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0inoWNP4x_z"
      },
      "outputs": [],
      "source": [
        "# we will save the conv layers and weights in these lists\n",
        "model_weights =[]\n",
        "conv_layers = []\n",
        "\n",
        "# get all the model children as list\n",
        "model_children = list(model.model.children())\n",
        "\n",
        "counter = 0\n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)):\n",
        "    if type(model_children[i]) == nn.Conv2d:\n",
        "        counter+=1\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])\n",
        "    elif type(model_children[i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[i])):\n",
        "            for child in model_children[i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter+=1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolution layers: {counter}\")\n",
        "print(\"conv_layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWT9UWpO41CO"
      },
      "outputs": [],
      "source": [
        "# transform an individual image to something we can push through the model\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=0., std=1.)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZBj6ZaU6ENJ"
      },
      "outputs": [],
      "source": [
        "# get a random image\n",
        "idx = random.randrange(0, len(label_data.patientId), 1)\n",
        "f = str(\"./project/stage_2_train_images/\" + label_data.patientId[idx] + \".dcm\")\n",
        "image = dcmread(f)\n",
        "plt.imshow(image.pixel_array, cmap=\"bone\")\n",
        "plt.axis('off')\n",
        "if label_data.Target[idx] == 1:\n",
        "  plt.title(\"Pneumonia\")\n",
        "else:\n",
        "  plt.title(\"Healthy\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# transform and load to GPU if available\n",
        "image = image.pixel_array\n",
        "image = image / 255.0\n",
        "\n",
        "image = (255*image).clip(0, 255).astype(np.uint8)\n",
        "image = Image.fromarray(image).convert('RGB')\n",
        "image = transform(image).to(device)\n",
        "outputs = []\n",
        "names = []\n",
        "for layer in conv_layers[0:]:\n",
        "  image = layer(image)\n",
        "  outputs.append(image)\n",
        "  names.append(str(layer))\n",
        "\n",
        "print(\"number of conv2d layers:\", len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9ty93J26Gp6"
      },
      "outputs": [],
      "source": [
        "processed = []\n",
        "for feature_map in outputs:\n",
        "  feature_map = feature_map.squeeze(0)\n",
        "  gray_scale = torch.sum(feature_map,0)\n",
        "  gray_scale = gray_scale / feature_map.shape[0]\n",
        "  processed.append(gray_scale.data.cpu().numpy())\n",
        "\n",
        "# display feature map after each conv2d layer\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i in range(12):\n",
        "  a = fig.add_subplot(4, 3, i+1)\n",
        "  imgplot = plt.imshow(processed[i], cmap=\"bone\")\n",
        "  a.axis(\"off\")\n",
        "  a.set_title(names[i].split('(')[0], fontsize=15)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}